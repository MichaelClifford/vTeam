# RHOAI AI Feature Sizing - Environment Variables Template
# Copy this file to .env and fill in your actual values

# ========================================
# CORE MODEL CONFIGURATION (REQUIRED)
# ========================================

# LLM inference model to use for feature planning
# Example: meta-llama/Llama-3.2-3B-Instruct, gpt-4o-mini, mistral-small
INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct

# URL of your Llama Stack server
# For local development: http://localhost:8321
# For OpenShift: http://llama-stack-service:8321
LLAMA_STACK_URL=http://localhost:8321

# ========================================
# JIRA INTEGRATION (via MCP Atlassian)
# ========================================

# MCP Atlassian service WebSocket URL for JIRA integration
# For local development: ws://localhost:3001/mcp
# For OpenShift: ws://mcp-atlassian-service:3001/mcp
MCP_ATLASSIAN_URL=ws://localhost:3001/mcp

# Whether to configure toolgroups (set to "true" for most setups)
CONFIGURE_TOOLGROUPS=true

# Optional: Filter JIRA projects (comma-separated project keys)
# JIRA_PROJECTS_FILTER=RHOAIENG,RHOAI

# ========================================
# DATABASE CONFIGURATION
# ========================================

# Option 1: PostgreSQL (Recommended for production)
# DATABASE_URL=postgresql://username:password@localhost:5432/rhoai_db

# Option 2: SQLite (Good for development/testing)
SQLITE_DB_PATH=./rhoai_sessions.db

# ========================================
# OPTIONAL GITHUB INTEGRATION
# ========================================

# GitHub Personal Access Token for accessing repositories in RAG
# Required only if you want to load GitHub repositories as knowledge sources
# GITHUB_ACCESS_TOKEN=ghp_your_token_here

# ========================================
# SERVER CONFIGURATION
# ========================================

# Port for the simple API server (default: 8001)
PORT=8001

# Host binding for the API server (default: 0.0.0.0)
HOST=0.0.0.0

# ========================================
# DEBUGGING & DEVELOPMENT
# ========================================

# Enable debug mode for database operations (true/false)
# DEBUG=false

# ========================================
# EXAMPLE CONFIGURATIONS
# ========================================

# Example 1: Local development with Ollama
# INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct
# LLAMA_STACK_URL=http://localhost:8321
# MCP_ATLASSIAN_URL=ws://localhost:3001/mcp
# SQLITE_DB_PATH=./rhoai_sessions.db

# Example 2: OpenShift deployment
# INFERENCE_MODEL=meta-llama/Llama-3.2-3B-Instruct  
# LLAMA_STACK_URL=http://llama-stack-service:8321
# MCP_ATLASSIAN_URL=ws://mcp-atlassian-service:3001/mcp
# DATABASE_URL=postgresql://user:pass@postgres-service:5432/rhoai_db

# Example 3: Using OpenAI API
# INFERENCE_MODEL=gpt-4o-mini
# LLAMA_STACK_URL=http://localhost:8321
# MCP_ATLASSIAN_URL=ws://localhost:3001/mcp
