# Secrets are created by deploy-to-openshift.sh script
# This includes llama-stack-secrets and jira-secrets
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
  namespace: llama-stack
data:
  run.yaml: |
    version: 2
    image_name: llamastack-rhoai-ai-feature-sizing
    container_image: llamastack-rhoai-ai-feature-sizing
    apis:
      - inference
      - safety
      - agents
      - vector_io
      - tool_runtime
      - telemetry
      - files
    providers:
      inference:
      - provider_id: vllm
        provider_type: remote::vllm
        config:
          url: ${env.VLLM_URL}
          api_token: ${env.VLLM_API_TOKEN:=fake}
      safety:
      - provider_id: llama-guard
        provider_type: inline::llama-guard
        config:
          excluded_categories: []
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing}/responses_store.db
      vector_io:
      - provider_id: faiss
        provider_type: inline::faiss
        config:
          kvstore:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=/tmp}/vector_kvstore.db
      tool_runtime:
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config:
          url: "http://jira-mcp-service.llama-stack.svc.cluster.local:9000/sse"
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: "${env.OTEL_SERVICE_NAME:=\u200B}"
          sinks: ${env.TELEMETRY_SINKS:=console,sqlite}
          sqlite_db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing}/trace_store.db
      files:
      - provider_id: localfs
        provider_type: inline::localfs
        config:
          storage_dir: ${env.FILES_STORAGE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing/files}
          metadata_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing}/files_metadata.db
    metadata_store: null
    inference_store: null
    models:
    - metadata: {}
      model_id: ${env.VLLM_INFERENCE_MODEL}
      provider_id: vllm
      model_type: llm
    shields: []
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups:
      - toolgroup_id: mcp::atlassian
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: ${env.MCP_ATLASSIAN_URL}
    logging: null
    server:
      port: 8321
      tls_certfile: null
      tls_keyfile: null
      tls_cafile: null
      auth: null
      host: null
      quota: null
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack
  template:
    metadata:
      labels:
        app: llama-stack
    spec:
      containers:
      - name: llama-stack
        image: llamastack/distribution-starter:latest
        command: ["python", "-m", "llama_stack.distribution.server.server", "--config", "/app/run.yaml"]
        ports:
        - containerPort: 8321
        env:
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_URL
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_API_TOKEN
              optional: true
        - name: VLLM_INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_INFERENCE_MODEL
        - name: SQLITE_STORE_DIR
          value: "/tmp"
        - name: VECTOR_STORAGE_DIR
          value: "/tmp/vector_storage"
        - name: OTEL_SERVICE_NAME
          value: "llama-stack"
        - name: TELEMETRY_SINKS
          value: "console,sqlite"
        - name: FILES_STORAGE_DIR
          value: "/tmp/files"
        - name: MCP_ATLASSIAN_URL
          value: "http://jira-mcp-service.llama-stack.svc.cluster.local:9000/sse"
        volumeMounts:
        - name: config-volume
          mountPath: /app/run.yaml
          subPath: run.yaml
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: config-volume
        configMap:
          name: llama-stack-config
---
apiVersion: v1
kind: Service
metadata:
  name: llama-stack-service
  namespace: llama-stack
spec:
  selector:
    app: llama-stack
  ports:
  - port: 8321
    targetPort: 8321
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jira-mcp
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jira-mcp
  template:
    metadata:
      labels:
        app: jira-mcp
    spec:
      containers:
      - name: jira-mcp
        image: ghcr.io/sooperset/mcp-atlassian:latest
        args: ["--transport", "sse", "-vv"]
        ports:
        - containerPort: 9000
        env:
        - name: JIRA_URL
          valueFrom:
            secretKeyRef:
              name: jira-secrets
              key: JIRA_URL
        - name: JIRA_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: jira-secrets
              key: JIRA_API_TOKEN
        - name: JIRA_USERNAME
          valueFrom:
            secretKeyRef:
              name: jira-secrets
              key: JIRA_USERNAME
        - name: PORT
          value: "9000"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "250m"
---
apiVersion: v1
kind: Service
metadata:
  name: jira-mcp-service
  namespace: llama-stack
spec:
  selector:
    app: jira-mcp
  ports:
  - port: 9000
    targetPort: 9000
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rhoai-ai-feature-sizing-api
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rhoai-ai-feature-sizing-api
  template:
    metadata:
      labels:
        app: rhoai-ai-feature-sizing-api
    spec:
      containers:
      - name: rhoai-ai-feature-sizing-api
        image: llamastack/distribution-starter:latest
        command: ["python", "-m", "rhoai_ai_feature_sizing.run_api"]
        ports:
        - containerPort: 8000
        env:
        - name: PORT
          value: "8000"
        - name: HOST
          value: "0.0.0.0"
        - name: DATABASE_URL
          value: "sqlite:///tmp/rhoai_sessions.db"
        - name: LLAMA_STACK_URL
          value: "http://llama-stack-service.llama-stack.svc.cluster.local:8321"
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_URL
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_API_TOKEN
              optional: true
        - name: VLLM_INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_INFERENCE_MODEL
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_INFERENCE_MODEL
        - name: LOG_LEVEL
          value: "INFO"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: rhoai-ai-feature-sizing-api-service
  namespace: llama-stack
spec:
  selector:
    app: rhoai-ai-feature-sizing-api
  ports:
  - port: 8000
    targetPort: 8000
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: rhoai-ai-feature-sizing-api-route
  namespace: llama-stack
spec:
  to:
    kind: Service
    name: rhoai-ai-feature-sizing-api-service
  port:
    targetPort: 8000
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: llama-stack-route
  namespace: llama-stack
spec:
  to:
    kind: Service
    name: llama-stack-service
  port:
    targetPort: 8321 