# =============================================================================
# RHOAI AI Feature Sizing - Environment Configuration
# =============================================================================
# Copy this file to .env and update the values for your environment
# cp env.example .env

# =============================================================================
# JIRA CONFIGURATION
# =============================================================================
# Your Jira instance URL (without trailing slash)
JIRA_URL=https://issues.redhat.com

# Your Jira API token (generate from: https://id.atlassian.com/manage-profile/security/api-tokens)
JIRA_API_TOKEN=

JIRA_USERNAME=username@redhat.com

# =============================================================================
# MCP Config
# =============================================================================
# Atlassian mcp url
MCP_ATLASSIAN_URL=http://jira-mcp:9000/sse

# =============================================================================
# LLAMA STACK CONFIGURATION
# =============================================================================
# Ollama model to use (must be pulled/available in your Ollama instance)
INFERENCE_MODEL=llama3.2:3b

LLAMA_STACK_URL=http://localhost:8321

# Ollama server URL (running locally or on another host)
OLLAMA_URL=http://host.docker.internal:11434

# Llama Index setup
GITHUB_ACCESS_TOKEN=


# =============================================================================
# LOGGING AND DEBUG
# =============================================================================
# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Enable debug mode for the Python application
DEBUG=false

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
# Directory for output files (relative to container)
OUTPUT_DIR=/outputs
