# Secrets are created by deploy-to-openshift.sh script
# This includes llama-stack-secrets and jira-secrets
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
  namespace: llama-stack
data:
  config.yaml: |
    version: 2
    image_name: llamastack-rhoai-ai-feature-sizing
    container_image: llamastack-rhoai-ai-feature-sizing
    apis:
      - inference
      - safety
      - agents
      - vector_io
      - tool_runtime
      - telemetry
      - files
    providers:
      inference:
      - provider_id: vllm
        provider_type: remote::vllm
        config:
          url: ${env.VLLM_URL}
          api_token: ${env.VLLM_API_TOKEN:=fake}
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      safety:
      - provider_id: llama-guard
        provider_type: inline::llama-guard
        config:
          excluded_categories: []
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            namespace: null
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing}/responses_store.db
      vector_io:
      - provider_id: faiss
        provider_type: inline::faiss
        config:
          kvstore:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=/tmp}/vector_kvstore.db
      tool_runtime:
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
      telemetry:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          service_name: "${env.OTEL_SERVICE_NAME:=\u200B}"
          sinks: ${env.TELEMETRY_SINKS:=console,sqlite}
          sqlite_db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing}/trace_store.db
      files:
      - provider_id: localfs
        provider_type: inline::localfs
        config:
          storage_dir: ${env.FILES_STORAGE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing/files}
          metadata_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/llamastack-rhoai-ai-feature-sizing}/files_metadata.db
    metadata_store: null
    inference_store: null
    models:
    - metadata: {}
      model_id: ${env.VLLM_INFERENCE_MODEL}
      provider_id: vllm
      model_type: llm
    - metadata:
        embedding_dimension: 384
      model_id: all-MiniLM-L6-v2
      provider_id: sentence-transformers
      model_type: embedding
    shields: []
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups:
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      - toolgroup_id: mcp::atlassian
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: ${env.MCP_ATLASSIAN_URL}
    logging: null
    server:
      port: 8321
      tls_certfile: null
      tls_keyfile: null
      tls_cafile: null
      auth: null
      host: null
      quota: null
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack
  template:
    metadata:
      labels:
        app: llama-stack
    spec:
      containers:
      - name: llama-stack
        image: llamastack/distribution-starter:latest
        command: ["python", "-m", "llama_stack.core.server.server", "/app/config.yaml"]
        ports:
        - containerPort: 8321
        env:
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_URL
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_API_TOKEN
              optional: true
        - name: VLLM_INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_INFERENCE_MODEL
        - name: SQLITE_STORE_DIR
          value: "/tmp"
        - name: VECTOR_STORAGE_DIR
          value: "/tmp/vector_storage"
        - name: OTEL_SERVICE_NAME
          value: "llama-stack"
        - name: TELEMETRY_SINKS
          value: "console,sqlite"
        - name: FILES_STORAGE_DIR
          value: "/tmp/files"
        - name: MCP_ATLASSIAN_URL
          value: "http://jira-mcp-service.llama-stack.svc.cluster.local:9000/sse"
        volumeMounts:
        - name: config-volume
          mountPath: /app/config.yaml
          subPath: config.yaml
        resources:
          requests:
            memory: "1Gi"
            cpu: "250m"
          limits:
            memory: "4Gi"
            cpu: "1000m"
      volumes:
      - name: config-volume
        configMap:
          name: llama-stack-config
---
apiVersion: v1
kind: Service
metadata:
  name: llama-stack-service
  namespace: llama-stack
spec:
  selector:
    app: llama-stack
  ports:
  - port: 8321
    targetPort: 8321
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jira-mcp
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jira-mcp
  template:
    metadata:
      labels:
        app: jira-mcp
    spec:
      containers:
      - name: jira-mcp
        image: ghcr.io/sooperset/mcp-atlassian:latest
        args: ["--transport", "sse", "-vv"]
        ports:
        - containerPort: 9000
        env:
        - name: JIRA_URL
          valueFrom:
            secretKeyRef:
              name: jira-secrets
              key: JIRA_URL
        - name: JIRA_PERSONAL_TOKEN
          valueFrom:
            secretKeyRef:
              name: jira-secrets
              key: JIRA_PERSONAL_TOKEN
        - name: PORT
          value: "9000"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "250m"
---
apiVersion: v1
kind: Service
metadata:
  name: jira-mcp-service
  namespace: llama-stack
spec:
  selector:
    app: jira-mcp
  ports:
  - port: 9000
    targetPort: 9000
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rhoai-ai-feature-sizing
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rhoai-ai-feature-sizing
  template:
    metadata:
      labels:
        app: rhoai-ai-feature-sizing
    spec:
      containers:
      - name: rhoai-ai-feature-sizing
        image: quay.io/gkrumbach07/rhoai-ai-feature-sizing:latest
        # Uses combined frontend+backend image with nginx proxy
        ports:
        - containerPort: 80
        env:
        - name: PORT
          value: "8000"
        - name: HOST
          value: "0.0.0.0"
        - name: DATABASE_URL
          value: "sqlite:///tmp/rhoai_sessions.db"
        - name: LLAMA_STACK_URL
          value: "http://llama-stack-service.llama-stack.svc.cluster.local:8321"
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_URL
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_API_TOKEN
              optional: true
        - name: VLLM_INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_INFERENCE_MODEL
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secrets
              key: VLLM_INFERENCE_MODEL
        - name: LOG_LEVEL
          value: "INFO"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-secrets
              key: DATABASE_URL
        - name: NODE_ENV
          value: "production"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
---
apiVersion: v1
kind: Service
metadata:
  name: rhoai-ai-feature-sizing-service
  namespace: llama-stack
spec:
  selector:
    app: rhoai-ai-feature-sizing
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP

---
# PostgreSQL Persistent Volume Claim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgresql-pvc
  namespace: llama-stack
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
# PostgreSQL Database for RHOAI API
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgresql
  namespace: llama-stack
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgresql
  template:
    metadata:
      labels:
        app: postgresql
    spec:
      containers:
      - name: postgresql
        image: postgres:15
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: database-secrets
              key: POSTGRES_DB
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: database-secrets
              key: POSTGRES_USER
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: database-secrets
              key: POSTGRES_PASSWORD
        volumeMounts:
        - name: postgresql-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "250m"
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}
          initialDelaySeconds: 15
          periodSeconds: 5
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: postgresql-storage
        persistentVolumeClaim:
          claimName: postgresql-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: postgresql-service
  namespace: llama-stack
spec:
  selector:
    app: postgresql
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP
 